{
  "title": "-src_enhancer.py",
  "text": "## [[Tags]]\n[[--- üß¨ Por Clasificar]]\n\n```python\nimport json\nimport re\nimport unicodedata\nfrom pathlib import Path\nfrom typing import Callable, Tuple, Dict, List\n\nfrom src.enhancer_utils import acumular_stats, aplicar_diccionario\n\n__all__ = [\n    'reemplazar_cid_ascii', 'reparar_encoding', 'normalizar_unicode',\n    'reparar_palabras_partidas', 'reparar_cid', 'reparar_ocr_simbolos',\n    'marcar_fragmentos_dudosos', 'pipeline_hooked_enhancer', 'acumular_stats'\n]\n\n# === Funciones de reparaci√≥n sem√°ntica ===\n\ndef reemplazar_cid_ascii(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Elimina etiquetas 'cid:123' y cuenta cu√°ntas sustituciones realiz√≥.\n    \"\"\"\n    matches = re.findall(r'cid:\\d+', texto)\n    nuevo_texto = re.sub(r'cid:\\d+', '', texto)\n    return nuevo_texto, {'reemplazos_cid_ascii': len(matches)}\n\n\ndef reparar_encoding(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Corrige errores comunes de encoding (mojibake) en el texto.\n    \"\"\"\n    mapping = {\n        '√É¬°': '√°', '√É¬©': '√©', '√É¬≠': '√≠', '√É¬≥': '√≥', '√É¬∫': '√∫',\n        '√É¬±': '√±', '√É‚Äò': '√ë'\n    }\n    count = 0\n    nuevo = texto\n    for mal, bien in mapping.items():\n        ocurrencias = nuevo.count(mal)\n        if ocurrencias:\n            nuevo = nuevo.replace(mal, bien)\n            count += ocurrencias\n    return nuevo, {'reparaciones_encoding': count}\n\n\ndef normalizar_unicode(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Normaliza la forma Unicode a NFC para asegurar combinaci√≥n de caracteres.\n    \"\"\"\n    return unicodedata.normalize('NFC', texto), {}\n\n\ndef reparar_palabras_partidas(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Une palabras partidas al final de l√≠nea: 'pro-\\nducto' ‚Üí 'producto'.\n    \"\"\"\n    pattern = r'-\\s*\\n\\s*'\n    ocurrencias = len(re.findall(pattern, texto))\n    nuevo = re.sub(pattern, '', texto)\n    return nuevo, {'palabras_reparadas': ocurrencias}\n\n\ndef reparar_cid(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Elimina patrones 'cid(123)' y cuenta las sustituciones.\n    \"\"\"\n    matches = re.findall(r'cid\\(\\d+\\)', texto)\n    nuevo = re.sub(r'cid\\(\\d+\\)', '', texto)\n    return nuevo, {'reemplazos_cid': len(matches)}\n\n\ndef reparar_ocr_simbolos(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Sustituye ligaduras y s√≠mbolos OCR por caracteres ASCII est√°ndar.\n    Ejemplo: 'Ô¨Å' ‚Üí 'fi', 'Ô¨Ç' ‚Üí 'fl', '∆ü' ‚Üí 'O'.\n    \"\"\"\n    mapping = {'Ô¨Å': 'fi', 'Ô¨Ç': 'fl', '∆ü': 'O'}\n    count = 0\n    nuevo = texto\n    for mal, bien in mapping.items():\n        ocurrencias = nuevo.count(mal)\n        if ocurrencias:\n            nuevo = nuevo.replace(mal, bien)\n            count += ocurrencias\n    return nuevo, {'reparados_ocr_simbolos': count}\n\n\ndef marcar_fragmentos_dudosos(texto: str) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Marca fragmentos dudosos sin alterar el contenido,\n    a√±adiendo etiquetas para an√°lisis posterior.\n    \"\"\"\n    # Implementaci√≥n m√≠nima: no-op, sin estad√≠sticas\n    return texto, {}\n\n\ndef pipeline_hooked_enhancer(\n    texto: str,\n    config: Dict[str, any]\n) -> Tuple[str, Dict[str, float]]:\n    \"\"\"\n    üß¨ Enhancer adaptativo con hooks inteligentes.\n\n    Aplica funciones de reparaci√≥n sem√°ntica en orden definido y de forma adaptativa:\n    - Mide el impacto de cada paso (cu√°ntas correcciones aplic√≥).\n    - Reintenta los que superan cierto umbral de ganancia.\n    - Carga un diccionario OCR externo si est√° disponible.\n\n    Args:\n        texto: texto original a mejorar.\n        config: diccionario con:\n            pasos: lista de funciones (func) que devuelven (texto, stats).\n            retry_umbral: umbral m√≠nimo de impacto para reintentar funci√≥n.\n            max_intentos: m√°ximo de repeticiones por funci√≥n.\n            ocr_dict_path: ruta opcional al archivo ocr_dict.json.\n\n    Returns:\n        texto corregido y estad√≠sticas acumuladas.\n    \"\"\"\n    pasos: List[Callable] = config.get('pasos', [])\n    retry_umbral: float = config.get('retry_umbral', 5)\n    max_intentos: int = config.get('max_intentos', 2)\n    stats_global: Dict[str, float] = {}\n    intentos: Dict[str, int] = {func.__name__: 0 for func in pasos}\n\n    # Cargar diccionario OCR si existe\n    dict_path = config.get('ocr_dict_path')\n    if dict_path and Path(dict_path).exists():\n        diccionario = json.loads(Path(dict_path).read_text(encoding='utf-8'))\n        pasos.insert(0, lambda t: aplicar_diccionario(t, diccionario))\n        intentos['<ocr_dict>'] = 0\n\n    # Loop adaptativo\n    cambios = True\n    texto_actual = texto\n    while cambios:\n        cambios = False\n        for func in pasos:\n            nombre = getattr(func, '__name__', '<ocr_dict>')\n            if intentos.get(nombre, 0) >= max_intentos:\n                continue\n\n            texto_nuevo, stats = func(texto_actual)\n            impacto = sum(v for v in stats.values() if isinstance(v, (int, float)))\n            if impacto >= retry_umbral:\n                cambios = True\n                intentos[nombre] += 1\n                texto_actual = texto_nuevo\n                stats_global = acumular_stats(stats_global, stats)\n\n    return texto_actual, stats_global\n\n```",
  "tags": "[[--- üß¨ Por Clasificar]]",
  "type": "text/markdown",
  "created": "20250424180404616",
  "modified": "20250424180404616"
}